{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05955b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ipynb #to use other python notebooks from the same folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555dc3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from tensorflow import keras\n",
    "from ipynb.fs.full.load_data import load_data_prediction\n",
    "from ipynb.fs.full.validation_code import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70c8d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3afb446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_params():\n",
    "    batch_size=int(input(\"Batch size: \"))\n",
    "    embedding_size=int(input(\"Embedding size: \"))\n",
    "    encoder_layers  = int(input(\"Number of encoder layers: \"))\n",
    "    decoder_layers  = int(input(\"Number of decoder layers: \"))\n",
    "    hidden_layer_size  = int(input(\"Size of hidden layer: \"))\n",
    "    cell_type=(input(\"Enter the cell type -lstm/rnn/gru \"))\n",
    "    drop= input(\"Incorporate Dropout? - y/n \")\n",
    "    if (drop=='y'):\n",
    "        dropout=float(input(\"Enter dropout rate: \"))\n",
    "    else:\n",
    "        dropout=0                          \n",
    "    print(\"\\n\")\n",
    "    beam_size= int(input(\"Enter beam width - beam width=0 enables a standard decoder\")) \n",
    "    epochs=int(input(\"Enter the number of epochs: \"))\n",
    "    return batch_size,embedding_size,encoder_layers,decoder_layers,hidden_layer_size,cell_type,drop,dropout,beam_size,epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f42e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test():\n",
    "\n",
    "    # Load train, val and test data. Also load the encoder and decoder properties\n",
    "    (encoder_train_input_data, decoder_train_input_data, decoder_train_target_data), (encoder_val_input_data, decoder_val_input_data, decoder_val_target_data), (val_input_words, val_target_words), (encoder_test_input_data, test_input_words, test_target_words), (num_encoder_characters, num_decoder_characters, max_encoder_seq_length, max_decoder_seq_length), (target_characters_index, inverse_target_characters_index) = load_data_split()\n",
    "    #obtain parameters\n",
    "    batch_size,embedding_size,encoder_layers,decoder_layers,hidden_layer_size,cell_type,drop,dropout,beam_size,epochs=obtain_params()\n",
    "\n",
    "\n",
    "    enc_dims = [hidden_layer_size] * encoder_layers\n",
    "    dec_dims  = [hidden_layer_size] * decoder_layers\n",
    "    cell_type = cell_type\n",
    "    dropout = dropout\n",
    "    beam_size = beam_size\n",
    "\n",
    "    # Encoder\n",
    "    encoder_inputs = keras.Input(shape = (None, ))\n",
    "    encoder_outputs = keras.layers.Embedding(input_dim = num_encoder_characters, output_dim = embedding_size, input_length = max_encoder_seq_length)(encoder_inputs)\n",
    "\n",
    "    # Encoder LSTM layers\n",
    "    encoder_states = list()\n",
    "    for j in range(len(enc_dims)):\n",
    "        if cell_type == \"rnn\":\n",
    "            encoder_outputs, state = keras.layers.SimpleRNN(enc_dims[j], dropout = dropout, return_state = True, return_sequences = True)(encoder_outputs)\n",
    "            encoder_states = [state]\n",
    "        if cell_type == \"lstm\":\n",
    "            encoder_outputs, state_h, state_c = keras.layers.LSTM(enc_dims[j], dropout = dropout, return_state = True, return_sequences = True)(encoder_outputs)\n",
    "            encoder_states = [state_h,state_c]\n",
    "        if cell_type == \"gru\":\n",
    "            encoder_outputs, state = keras.layers.GRU(enc_dims[j], dropout = dropout, return_state = True, return_sequences = True)(encoder_outputs)\n",
    "            encoder_states = [state]\n",
    "\n",
    "    # Decoder\n",
    "    decoder_inputs = keras.Input(shape=(None, ))\n",
    "    decoder_outputs = keras.layers.Embedding(input_dim = num_decoder_characters, output_dim = embedding_size, input_length = max_decoder_seq_length)(decoder_inputs)\n",
    "\n",
    "    decoder_states = encoder_states.copy()\n",
    "\n",
    "    for j in range(len(dec_dims)):\n",
    "        if cell_type == \"rnn\": \n",
    "            decoder = keras.layers.SimpleRNN(dec_dims[j], dropout = dropout, return_sequences = True, return_state = True)\n",
    "            decoder_outputs, state = decoder(decoder_outputs, initial_state = decoder_states)\n",
    "\n",
    "        if cell_type == \"lstm\":\n",
    "            decoder = keras.layers.LSTM(dec_dims[j], dropout = dropout, return_sequences = True, return_state = True)\n",
    "            decoder_outputs, state_h, state_c = decoder(decoder_outputs, initial_state = decoder_states)\n",
    "\n",
    "        if cell_type == \"gru\":\n",
    "            decoder = keras.layers.GRU(dec_dims[j], dropout = dropout, return_sequences = True, return_state = True)\n",
    "            decoder_outputs, state = decoder(decoder_outputs, initial_state = decoder_states)\n",
    "\n",
    "\n",
    "    decoder_dense = keras.layers.Dense(num_decoder_characters, activation = \"softmax\")\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Model definition\n",
    "    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    model.fit([encoder_train_input_data, decoder_train_input_data],\n",
    "        decoder_train_target_data,\n",
    "        batch_size = batch_size,\n",
    "        epochs = epochs)\n",
    "\n",
    "    # print('train done')\n",
    "\n",
    "    # Inference Call for Validation Data\n",
    "    #estimates the validation accuracy and the number of correct predictions in the validation dataset\n",
    "    val_accuracy, count1 = valid_func(model,encoder_val_input_data, val_input_words, val_target_words, max_decoder_seq_length,max_encoder_seq_length, target_characters_index, inverse_target_characters_index, enc_dims, dec_dims, cell_type, beam_size)\n",
    "    print(\"Validation accuracy\",val_accuracy\"\\n\")\n",
    "    print(\"Number of correct guesses\",count1,\"\\n\")\n",
    "\n",
    "    # Inference Call for Test Data. comment the following two lines when training the model.\n",
    "    test_accuracy,count1 = valid_func(model,encoder_test_input_data, test_input_words, test_target_words, max_decoder_seq_length, max_encoder_seq_length,target_characters_index, inverse_target_characters_index, enc_dims, dec_dims, cell_type, beam_size)\n",
    "    print(\"test accuracy\",test_accuracy\"\\n\")\n",
    "    print(\"Number of correct guesses\",count1,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd31f06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wandb sweep configuration \n",
    "sweep_config = {\n",
    "  \n",
    "  \"method\": \"random\",\n",
    "\n",
    "  'metric': {\n",
    "      'name': 'accuracy',\n",
    "      'goal': 'maximize'\n",
    "  },\n",
    "\n",
    "  \"parameters\": {\n",
    "        \"embedding_size\": {\n",
    "            \"values\": [256]\n",
    "        },\n",
    "        \"encoder_layers\" :{\n",
    "            \"values\" : [3]\n",
    "        },\n",
    "        \"decoder_layers\": {\n",
    "            \"values\": [1]\n",
    "        },\n",
    "        \"hidden_layer_size\": {\n",
    "            \"values\": [256]\n",
    "        },\n",
    "        \"cell_type\": {\n",
    "            \"values\": [\"lstm\"]\n",
    "        },\n",
    "        \"dropout\": {\n",
    "            \"values\": [0.2]\n",
    "        },\n",
    "        \"beam_size\": {\n",
    "            \"values\": [4]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"Assignment 3 test data\")\n",
    "wandb.agent(sweep_id, project = \"Assignment 3 test data\", function = main,count=1)\n",
    "\n",
    "# Load train, val and test data. Also load the encoder and decoder properties\n",
    "(encoder_train_input_data, decoder_train_input_data, decoder_train_target_data), (encoder_val_input_data, decoder_val_input_data, decoder_val_target_data), (val_input_words, val_target_words), (encoder_test_input_data, test_input_words, test_target_words), (num_encoder_characters, num_decoder_characters, max_encoder_seq_length, max_decoder_seq_length), (target_characters_index, inverse_target_characters_index) = load_data_split()\n",
    "\n",
    "def main(config = None):\n",
    "    # wandb config\n",
    "    run = wandb.init(config = config)\n",
    "    config = wandb.config\n",
    "\n",
    "    run.name = \"Embedding Size: \" + str(config.embedding_size) + \" Cell Type: \" + config.cell_type + \" Dropout: \" + str(config.dropout) + \" Beam Size: \" + str(config.beam_size) + \" Encoder Layers: \" + str(config.encoder_layers) + \" Decoder Layers: \" + str(config.decoder_layers) + \" Hidden Layer Size: \" + str(config.hidden_layer_size)\n",
    "\n",
    "    # Parameters\n",
    "    batch_size = 128\n",
    "    epochs = 20\n",
    "    embedding_size = config.embedding_size\n",
    "    enc_dims = [config.hidden_layer_size] * config.encoder_layers\n",
    "    dec_dims  = [config.hidden_layer_size] * config.decoder_layers\n",
    "    cell_type = config.cell_type\n",
    "    dropout = config.dropout\n",
    "    beam_size = config.beam_size\n",
    "\n",
    "    # Encoder\n",
    "    encoder_inputs = keras.Input(shape = (None, ))\n",
    "    encoder_outputs = keras.layers.Embedding(input_dim = num_encoder_characters, output_dim = embedding_size, input_length = max_encoder_seq_length)(encoder_inputs)\n",
    "\n",
    "    # Encoder LSTM layers\n",
    "    encoder_states = list()\n",
    "    for j in range(len(enc_dims)):\n",
    "        if cell_type == \"rnn\":\n",
    "            encoder_outputs, state = keras.layers.SimpleRNN(enc_dims[j], dropout = dropout, return_state = True, return_sequences = True)(encoder_outputs)\n",
    "            encoder_states = [state]\n",
    "        if cell_type == \"lstm\":\n",
    "            encoder_outputs, state_h, state_c = keras.layers.LSTM(enc_dims[j], dropout = dropout, return_state = True, return_sequences = True)(encoder_outputs)\n",
    "            encoder_states = [state_h,state_c]\n",
    "        if cell_type == \"gru\":\n",
    "            encoder_outputs, state = keras.layers.GRU(enc_dims[j], dropout = dropout, return_state = True, return_sequences = True)(encoder_outputs)\n",
    "            encoder_states = [state]\n",
    "\n",
    "    # Decoder\n",
    "    decoder_inputs = keras.Input(shape=(None, ))\n",
    "    decoder_outputs = keras.layers.Embedding(input_dim = num_decoder_characters, output_dim = embedding_size, input_length = max_decoder_seq_length)(decoder_inputs)\n",
    "\n",
    "    decoder_states = encoder_states.copy()\n",
    "\n",
    "    for j in range(len(dec_dims)):\n",
    "        if cell_type == \"rnn\": \n",
    "            decoder = keras.layers.SimpleRNN(dec_dims[j], dropout = dropout, return_sequences = True, return_state = True)\n",
    "            decoder_outputs, state = decoder(decoder_outputs, initial_state = decoder_states)\n",
    "           \n",
    "        if cell_type == \"lstm\":\n",
    "            decoder = keras.layers.LSTM(dec_dims[j], dropout = dropout, return_sequences = True, return_state = True)\n",
    "            decoder_outputs, state_h, state_c = decoder(decoder_outputs, initial_state = decoder_states)\n",
    "            \n",
    "        if cell_type == \"gru\":\n",
    "            decoder = keras.layers.GRU(dec_dims[j], dropout = dropout, return_sequences = True, return_state = True)\n",
    "            decoder_outputs, state = decoder(decoder_outputs, initial_state = decoder_states)\n",
    "            \n",
    "\n",
    "    decoder_dense = keras.layers.Dense(num_decoder_characters, activation = \"softmax\")\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    #create models\n",
    "    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]) \n",
    "\n",
    "    model.fit(\n",
    "        [encoder_train_input_data, decoder_train_input_data],\n",
    "        decoder_train_target_data,\n",
    "        batch_size = batch_size,\n",
    "        epochs = epochs,\n",
    "        callbacks = [WandbCallback()])\n",
    "\n",
    "   # print('train done')\n",
    "\n",
    "    # Inference Call for Validation Data\n",
    "    #estimates the validation accuracy and the number of correct predictions in the validation dataset\n",
    "    val_accuracy, count1 = valid_func(model,encoder_val_input_data, val_input_words, val_target_words, max_decoder_seq_length,max_encoder_seq_length, target_characters_index, inverse_target_characters_index, enc_dims, dec_dims, cell_type, beam_size)\n",
    "    wandb.log( { \"val_accuracy\": val_accuracy})\n",
    "    wandb.log({\"count\": count1})\n",
    "\n",
    "    # Inference Call for Test Data, comment the following two lines when training the model.\n",
    "    test_accuracy,count1 = valid_func(model,encoder_test_input_data, test_input_words, test_target_words, max_decoder_seq_length, max_encoder_seq_length,target_characters_index, inverse_target_characters_index, enc_dims, dec_dims, cell_type, beam_size)\n",
    "    wandb.log( { \"test_accuracy\": test_accuracy} )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
