{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9134951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "# Function to calculate indices of layers\n",
    "def get_layer_index(model, enc_dims, cell_type):\n",
    "\n",
    "    n_enc_layer = len(enc_dims)\n",
    "\n",
    "    e_emb_index, d_emb_index = -1, -1\n",
    "    e_cell_index, d_cell_index = [], []\n",
    "    dense_index = -1\n",
    "\n",
    "    e_celllayer_count = 0\n",
    "    \n",
    "    for i, layer in enumerate(model.layers):\n",
    "        # Dense Layer\n",
    "        if \"dense\" in layer.name :\n",
    "            dense_index = i\n",
    "\n",
    "        # Embedding layers\n",
    "        if \"embedding\" in layer.name:\n",
    "            if e_emb_index == -1 :\n",
    "                e_emb_index = i\n",
    "            else:\n",
    "                d_emb_index = i\n",
    "\n",
    "        # Lstm layers\n",
    "        if cell_type in layer.name:\n",
    "            if e_celllayer_count < n_enc_layer:\n",
    "                e_cell_index.append(i)\n",
    "                e_celllayer_count += 1\n",
    "            else:\n",
    "                d_cell_index.append(i)\n",
    "\n",
    "    return e_emb_index, d_emb_index, e_cell_index, d_cell_index, dense_index\n",
    "\n",
    "# function to perform the word comparison \n",
    "def valid_func(model,encoder_input_data,input_words,target_words, max_decoder_seq_length, max_encoder_seq_length, target_characters_index, inverse_target_characters_index, enc_dims, dec_dims, cell_type, beam_size):\n",
    "    \n",
    "    #get the layer indices\n",
    "    e_emb_index, d_emb_index, e_cell_index, d_cell_index, dense_index = get_layer_index(model, enc_dims, cell_type)\n",
    "\n",
    "    # Encoder\n",
    "    encoder_inputs = model.input[0]  \n",
    "\n",
    "    if cell_type == \"rnn\" or cell_type == \"gru\":\n",
    "        encoder_outputs, state = model.layers[e_cell_index[-1]].output\n",
    "        encoder_model = keras.Model(encoder_inputs, [state])\n",
    "    \n",
    "    elif cell_type == \"lstm\":\n",
    "        encoder_outputs, state_h_enc, state_c_enc = model.layers[e_cell_index[-1]].output\n",
    "        encoder_model = keras.Model(encoder_inputs, [state_h_enc, state_c_enc])\n",
    "    \n",
    "    else:\n",
    "        return\n",
    "\n",
    "    decoder_inputs = model.input[1]  \n",
    "    decoder_outputs =  model.layers[d_emb_index](decoder_inputs)\n",
    "\n",
    "    decoder_states_inputs =  []\n",
    "    decoder_states = []\n",
    "\n",
    "    # Decoder LSTM\n",
    "    for dec in range(len(d_cell_index)):\n",
    "        \n",
    "        if cell_type == \"rnn\" or cell_type == \"gru\":\n",
    "            state = keras.Input(shape = (dec_dims[dec], ))\n",
    "            current_states_inputs = [state]\n",
    "            decoder_outputs, state = model.layers[d_cell_index[dec]](decoder_outputs, initial_state = current_states_inputs)\n",
    "            decoder_states += [state]\n",
    "\n",
    "        elif cell_type == \"lstm\":\n",
    "            state_h_dec, state_c_dec = keras.Input(shape = (dec_dims[dec],)),  keras.Input(shape = (dec_dims[dec],))\n",
    "            current_states_inputs = [state_h_dec, state_c_dec]\n",
    "            decoder_outputs, state_h_dec,state_c_dec = model.layers[d_cell_index[dec]](decoder_outputs, initial_state = current_states_inputs)\n",
    "            decoder_states += [state_h_dec, state_c_dec]\n",
    "        \n",
    "        decoder_states_inputs += current_states_inputs\n",
    "\n",
    "    # Dense layer\n",
    "    decoder_dense = model.layers[dense_index]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Final decoder model\n",
    "    decoder_model = keras.Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "\n",
    "    def standard_decoder(input_seq):\n",
    "        # Encode the input as state vectors.\n",
    "        states_value = [encoder_model.predict(input_seq)] * len(d_cell_index)\n",
    "\n",
    "        # Generate empty target sequence\n",
    "        targ_seq = np.zeros((1,1))\n",
    "\n",
    "        targ_seq[0, 0 ] = target_characters_index[\"\\t\"]\n",
    "        \n",
    "        stop_condition = False\n",
    "        decoded_sentence = \"\"\n",
    "\n",
    "        while not stop_condition:\n",
    "            output = decoder_model.predict([targ_seq] + states_value)\n",
    "            output_tokens, states_value = output[0], output[1:]\n",
    "\n",
    "            # Sample a token\n",
    "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "\n",
    "            sampled_char = inverse_target_characters_index[sampled_token_index]\n",
    "\n",
    "            decoded_sentence += sampled_char\n",
    "\n",
    "            if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "                stop_condition = True\n",
    "\n",
    "            targ_seq = np.zeros((1, 1))\n",
    "            targ_seq[0, 0] = sampled_token_index \n",
    "            \n",
    "\n",
    "        return decoded_sentence\n",
    "\n",
    "    def beam_search(input_seq, k):\n",
    "        # Encode the input as state vectors.\n",
    "        states_value = [encoder_model.predict(input_seq)] * len(d_cell_index)\n",
    "\n",
    "        # Generate empty target sequence\n",
    "        targ_seq = np.zeros((1, 1))\n",
    "\n",
    "        \n",
    "        targ_seq[0, 0 ] = target_characters_index[\"\\t\"]\n",
    "        \n",
    "        stop_condition = False\n",
    "        decoded_sentence = \"\"\n",
    "\n",
    "        sequences = [[0.0, 0, states_value, targ_seq,  list(),list()]]\n",
    "\n",
    "        while not stop_condition:\n",
    "\n",
    "            poss = list()\n",
    "            \n",
    "            for i in range(len(sequences)):\n",
    "                output = decoder_model.predict([sequences[i][3]] + sequences[i][2])\n",
    "                output_tokens, states_value = output[0], output[1:]\n",
    "                prob = output_tokens[0,-1,:]\n",
    "              \n",
    "                score, eow, sv, t_seq, seq, d_word = sequences[i]\n",
    "              \n",
    "                if eow == 0:\n",
    "                    for j in range(len(inverse_target_characters_index)):\n",
    "                        char = inverse_target_characters_index[j]\n",
    "\n",
    "                        targ_seq = np.zeros((1, 1))\n",
    "                        targ_seq[0, 0] = j\n",
    "\n",
    "                        candidate = [score - np.log(prob[j]), 0, states_value, targ_seq,  seq + [j] , d_word + [char] ]\n",
    "                        poss.append(candidate)\n",
    "            \n",
    "            ordered = sorted(poss, key=lambda x:x[0])\n",
    "\n",
    "            min_length = min(k, len(ordered))\n",
    "\n",
    "            sequences = ordered[:min_length]\n",
    "\n",
    "            stop_condition = True\n",
    "           \n",
    "            for sequence in range(len(sequences)):\n",
    "               \n",
    "                score, eow, sv, t_seq, seq, d_word = sequences[sequence]\n",
    "\n",
    "                if d_word[-1] == \"\\n\": eow = 1\n",
    "\n",
    "                if len(d_word) > max_decoder_seq_length : eow = 1\n",
    "\n",
    "                sequences[sequence] = [score, eow, sv, t_seq, seq, d_word].copy()\n",
    "\n",
    "                if eow == 0: stop_condition = False\n",
    "\n",
    "            if sequences[0][-1][-1]==\"\\n\": stop_condition = True\n",
    "\n",
    "        best_sentence = ''.join(sequences[0][5])\n",
    "\n",
    "        return best_sentence\n",
    "\n",
    "    count, input_size = 0, len(input_words)\n",
    "    \n",
    "\n",
    "    predictions_vanilla = open(\"predictions_vanilla.csv\", \"w\", encoding='utf-8') # Opens csv file to store predictions\n",
    "    predictions_vanilla.write(\"Input Sentence,Predicted Sentence,Original Sentence\\n\")  # Header for the csv file \n",
    "    \n",
    "\n",
    "\n",
    "    for seq_index in range(input_size):      #time consuming to go through the entire val data! More so for beam search   \n",
    "        input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
    "        \n",
    "        if beam_size == 0:\n",
    "            # Standard decoder\n",
    "            decoded_word= standard_decoder(input_seq)\n",
    "        else:\n",
    "           #beam search decoder \n",
    "            decoded_word = beam_search(input_seq, beam_size)\n",
    "\n",
    "        orig_word = target_words[seq_index][1:]\n",
    "    \n",
    "        predictions_vanilla.write(input_words[seq_index] + \",\" + decoded_word[:-1] + \",\" + orig_word[:-1] + \"\\n\") # write the predictions\n",
    "        \n",
    "        if(orig_word == decoded_word): \n",
    "            count += 1\n",
    "            \n",
    "    val_acc=count / input_size\n",
    "    return val_acc,count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f16bc1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f59cfd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
