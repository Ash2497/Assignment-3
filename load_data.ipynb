{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52894896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data_split():\n",
    "    \n",
    "    # Read the files using the file location\n",
    "    train_data = pd.read_csv(\"dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\", sep = '\\t', header = None)\n",
    "    val_data = pd.read_csv(\"dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\", sep = '\\t', header = None)\n",
    "    test_data = pd.read_csv(\"dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\", sep = '\\t', header = None)\n",
    "\n",
    "    train_input = [str(word) for word in list(train_data[1])]\n",
    "    train_target = [\"\\t\" + str(word) + \"\\n\" for word in list(train_data[0])]\n",
    "\n",
    "    val_input = [str(word) for word in list(val_data[1])]\n",
    "    val_target = [\"\\t\" + str(word) + \"\\n\" for word in list(val_data[0])]\n",
    "\n",
    "    test_input = [str(word) for word in list(test_data[1])]\n",
    "    test_target = [\"\\t\" + str(word) + \"\\n\" for word in list(test_data[0])]\n",
    "\n",
    "    input_characters = set()\n",
    "    target_characters = set()\n",
    "\n",
    "    input_characters.add(' ')\n",
    "    target_characters.add(' ')\n",
    "\n",
    "    for train_word in train_input:\n",
    "        for char in train_word:\n",
    "            input_characters.add(char)\n",
    "\n",
    "    for val_word in val_input:\n",
    "        for char in val_word:\n",
    "            input_characters.add(char)\n",
    "\n",
    "    for test_word in test_input:\n",
    "        for char in test_word:\n",
    "            input_characters.add(char)\n",
    "\n",
    "    for train_word in train_target:\n",
    "        for char in train_word:\n",
    "            target_characters.add(char)\n",
    "\n",
    "    for val_word in val_target:\n",
    "        for char in val_word:\n",
    "            target_characters.add(char)\n",
    "\n",
    "    for test_word in test_target:\n",
    "        for char in test_word:\n",
    "            target_characters.add(char)\n",
    "\n",
    "    input_characters       = sorted(list(input_characters))\n",
    "    target_characters      = sorted(list(target_characters))\n",
    "    num_encoder_characters = len(input_characters)\n",
    "    num_decoder_characters = len(target_characters)\n",
    "\n",
    "    max_encoder_seq_length = max(max([len(word) for word in train_input]), max([len(word) for word in val_input]), max([len(word) for word in test_input]))\n",
    "    max_decoder_seq_length = max(max([len(word) for word in train_target]), max([len(word) for word in val_target]), max([len(word) for word in test_target]))\n",
    "\n",
    "    # Summary of the input data\n",
    "    print(\"Number of train words: \", len(train_input))\n",
    "    print(\"Number of val words: \", len(val_input))\n",
    "    print(\"Number of test words: \", len(test_input))\n",
    "    print(\"Number of input characters: \", num_encoder_characters)\n",
    "    print(\"Number of output characters: \", num_decoder_characters)\n",
    "    print(\"Max sequence length for inputs: \", max_encoder_seq_length)\n",
    "    print(\"Max sequence length for train outputs: \", max_decoder_seq_length)\n",
    "\n",
    "    input_characters_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "    target_characters_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "    inverse_input_characters_index = dict((i, char) for char, i in input_characters_index.items())\n",
    "    inverse_target_characters_index = dict((i, char) for char, i in target_characters_index.items())\n",
    "\n",
    "    encoder_train_input_data = np.zeros(\n",
    "        (len(train_input), max_encoder_seq_length), dtype=\"float32\"\n",
    "    )\n",
    "    decoder_train_input_data = np.zeros(\n",
    "        (len(train_input), max_decoder_seq_length), dtype=\"float32\"\n",
    "    )\n",
    "    decoder_train_target_data = np.zeros(\n",
    "        (len(train_input), max_decoder_seq_length, num_decoder_characters ), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    encoder_val_input_data = np.zeros(\n",
    "        (len(val_input), max_encoder_seq_length), dtype=\"float32\"\n",
    "    )\n",
    "    decoder_val_input_data = np.zeros(\n",
    "        (len(val_input), max_decoder_seq_length), dtype=\"float32\"\n",
    "    )\n",
    "    decoder_val_target_data = np.zeros(\n",
    "        (len(val_input), max_decoder_seq_length, num_decoder_characters), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    encoder_test_input_data = np.zeros(\n",
    "        (len(test_input), max_encoder_seq_length), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    for i, (input_word, target_word) in enumerate(zip(train_input, train_target)):\n",
    "        for t, char in enumerate(input_word):\n",
    "            encoder_train_input_data[i, t] = input_characters_index[char]\n",
    "        encoder_train_input_data[i, t + 1 :] = input_characters_index[' ']\n",
    "        \n",
    "        for t, char in enumerate(target_word):\n",
    "            decoder_train_input_data[i, t] = target_characters_index[char]\n",
    "            if t > 0:\n",
    "                decoder_train_target_data[i, t - 1, target_characters_index[char]] = 1.0\n",
    "        decoder_train_input_data[i, t + 1 :] = target_characters_index[' ']\n",
    "        decoder_train_target_data[i, t :, target_characters_index[' ']] = 1.0\n",
    "\n",
    "    for i, (input_word, target_word) in enumerate(zip(val_input, val_target)):\n",
    "        for t, char in enumerate(input_word):\n",
    "            encoder_val_input_data[i, t] = input_characters_index[char]\n",
    "        encoder_val_input_data[i, t + 1 :] = input_characters_index[' ']\n",
    "        \n",
    "        for t, char in enumerate(target_word):\n",
    "            decoder_val_input_data[i, t] = target_characters_index[char]\n",
    "            if t > 0:\n",
    "                decoder_val_target_data[i, t - 1 :, target_characters_index[char]] = 1.0\n",
    "        decoder_val_input_data[i, t + 1 :] =  target_characters_index[' ']\n",
    "        decoder_val_target_data[i, t :, target_characters_index[' ']] = 1.0\n",
    "\n",
    "    for i, input_word in enumerate(test_input):\n",
    "        for t, char in enumerate(input_word):\n",
    "            encoder_test_input_data[i, t] = input_characters_index[char]\n",
    "        encoder_test_input_data[i, t + 1 :] = input_characters_index[' ']\n",
    "\n",
    "\n",
    "    return (encoder_train_input_data, decoder_train_input_data, decoder_train_target_data), (encoder_val_input_data, decoder_val_input_data, decoder_val_target_data), (val_input, val_target), (encoder_test_input_data, test_input, test_target), (num_encoder_characters, num_decoder_characters, max_encoder_seq_length, max_decoder_seq_length), (target_characters_index, inverse_target_characters_index)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8536062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
